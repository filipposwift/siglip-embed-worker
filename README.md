# SigLIP2 Embedding Worker - RunPod Serverless - Test

Endpoint RunPod Serverless per generare embedding di immagini e testo usando il modello SigLIP2, ottimizzato per similarity search cross-modal (testo-immagini) su costumi da bagno con pattern e texture complessi.

## ğŸ¯ Caratteristiche

- **Modello**: `google/siglip2-large-patch16-512` (risoluzione 512x512 per massimi dettagli)
- **Embedding normalizzati L2**: Pronti per cosine similarity search
- **ModalitÃ  duali**: Supporta embedding sia di immagini che di testo nello stesso spazio vettoriale
- **Similarity search cross-modal**: Cerca immagini usando query testuali (es. "bikini top with floral pattern")
- **Batch processing**: Elabora multiple immagini in parallelo
- **Gestione errori robusta**: Continua l'elaborazione anche se alcune immagini falliscono
- **Ottimizzato per GPU**: Utilizza CUDA su GPU NVIDIA (4090 o superiore)

## ğŸ“‹ Requisiti

- RunPod Serverless account
- GPU NVIDIA con CUDA 12.8 (4090, A5000, RTX 50, ecc.)
- Repository GitHub collegato a RunPod

## ğŸ—ï¸ Architettura

### Struttura del Progetto

```
siglip-embed-worker/
â”œâ”€â”€ Dockerfile              # Immagine Docker per RunPod
â”œâ”€â”€ requirements.txt        # Dipendenze Python
â”œâ”€â”€ rp_handler.py          # Handler principale RunPod Serverless
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py           # Caricamento e inferenza SigLIP2
â”‚   â””â”€â”€ image_io.py        # Download e preprocessing immagini
â””â”€â”€ README.md
```

### Scelte Architetturali

#### 1. **Modello SigLIP2 Large 512x512**

**Scelta**: `google/siglip2-large-patch16-512` invece di `siglip2-base-patch16-256`

**Motivazione**:

- **Dettagli superiori**: Risoluzione 512x512 cattura pattern e texture piÃ¹ fini
- **Similarity search migliore**: Embedding piÃ¹ ricchi e discriminanti per distinguere costumi simili
- **Caso d'uso specifico**: Per costumi da bagno con pattern complessi, i dettagli extra sono cruciali

**Trade-off**:

- âœ… QualitÃ  embedding significativamente migliore
- âš ï¸ Modello piÃ¹ pesante (~2-4x piÃ¹ lento, piÃ¹ memoria)
- âš ï¸ Costo RunPod leggermente superiore

#### 2. **Resize Intermedio a 2048px**

**Scelta**: Resize intermedio a max 2048px prima del processor SigLIP2

**Motivazione**:

- **Protezione memoria**: Immagini enormi (4000x6000) vengono ridotte a ~8MB invece di 69MB
- **Dettagli massimi**: Per immagini tipiche (~2700px) perdita solo ~24% vs 62% con 1024px
- **Zero perdita per immagini piccole**: Immagini < 2048px non vengono toccate

**Flusso**:

```
Immagine originale (es. 1800x2699 o 4000x6000)
    â†“
Resize intermedio a max 2048px (mantiene dettagli massimi)
    â†“
AutoProcessor SigLIP2 resize a 512x512 (con padding/aspect ratio)
    â†“
Embedding normalizzato L2
```

**Analisi**:

- Immagine tipica 1800x2699: 13.9MB â†’ 8MB (perdita 24%)
- Immagine grande 4000x6000: 69MB â†’ 8MB (risparmio 88%)

#### 3. **Normalizzazione L2 degli Embedding**

**Scelta**: Normalizzare tutti gli embedding a norma unitaria (L2)

**Motivazione**:

- **Cosine similarity = dot product**: Con embedding normalizzati, `np.dot(emb1, emb2)` = cosine similarity
- **Performance database vettoriali**: Molti database (Pinecone, Weaviate, Qdrant) ottimizzano per vettori normalizzati
- **StabilitÃ  numerica**: Riduce problemi di overflow/underflow
- **Best practice**: Standard per embedding (CLIP, SigLIP, ecc.)

**Implementazione**:

```python
norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
embeddings = embeddings / norms  # Norma unitaria
```

#### 4. **Singleton Pattern per il Modello**

**Scelta**: Caricare modello e processor una sola volta (variabili globali)

**Motivazione**:

- **Performance**: Evita ricaricamento a ogni richiesta (modello ~1-2GB)
- **Memoria**: Un solo modello in memoria invece di multipli
- **VelocitÃ **: Prima richiesta piÃ¹ lenta (download modello), successive molto veloci

**Implementazione**:

```python
processor = None
model = None

def _load_model():
    global processor, model
    if processor is None or model is None:
        # Carica solo se non giÃ  caricato
```

#### 5. **Device Sempre CUDA**

**Scelta**: `device = torch.device("cuda")` senza fallback CPU

**Motivazione**:

- **RunPod garantisce GPU**: I pod vengono sempre attivati con GPU NVIDIA (4090 o superiore)
- **Performance**: CUDA Ã¨ 10-100x piÃ¹ veloce per inferenza deep learning
- **SemplicitÃ **: Nessun codice condizionale necessario

#### 6. **Gestione Errori Robusta**

**Scelta**: Continuare l'elaborazione anche se alcune immagini falliscono

**Motivazione**:

- **Resilienza**: Un URL non valido non blocca l'intero batch
- **Informazioni utili**: Restituisce warning con dettagli degli URL falliti
- **UX migliore**: L'utente riceve embedding per immagini valide + info su quelle fallite

**Implementazione**:

```python
for url in image_urls:
    try:
        image = load_image_from_url(url)
        images.append(image)
    except Exception as e:
        failed_urls.append({"url": url, "error": str(e)})
```

## ğŸš€ Deployment su RunPod Serverless

### 1. Preparazione Repository GitHub

1. Push del codice su GitHub
2. Assicurarsi che la struttura sia corretta:
   ```
   â”œâ”€â”€ Dockerfile
   â”œâ”€â”€ requirements.txt
   â”œâ”€â”€ rp_handler.py
   â””â”€â”€ src/
       â”œâ”€â”€ __init__.py
       â”œâ”€â”€ model.py
       â””â”€â”€ image_io.py
   ```

### 2. Configurazione RunPod

1. Accedi a [RunPod Console](https://www.runpod.io/console/serverless)
2. Vai a **Serverless** â†’ **New Endpoint**
3. Seleziona **GitHub Integration** e collega il repository
4. Configurazione:
   - **Handler**: `rp_handler.handler`
   - **Dockerfile path**: `Dockerfile` (root del repo)
   - **GPU**: NVIDIA 4090 o superiore (minimo 16GB VRAM)
   - **Container disk**: 20GB+ (per cache modello Hugging Face)

### 3. Build e Deploy

RunPod costruirÃ  automaticamente l'immagine Docker dal Dockerfile. Il primo deploy puÃ² richiedere 5-10 minuti per:

- Build immagine Docker
- Download modello SigLIP2 da Hugging Face (~1-2GB)
- Setup ambiente

## ğŸ“¡ Utilizzo API

### Endpoint

```
POST https://api.runpod.ai/v2/{endpoint_id}/runsync
```

### ModalitÃ  Immagini (mode="images")

#### Richiesta

```json
{
  "input": {
    "mode": "images",
    "payload": [
      "https://example.com/image1.jpg",
      "https://example.com/image2.png",
      "https://example.com/image3.jpg"
    ]
  }
}
```

#### Risposta di Successo

```json
{
  "status": "OK",
  "embeddings": [
    {
      "image_url": "https://example.com/image1.jpg",
      "vector": [0.123, 0.456, 0.789, ...]
    },
    {
      "image_url": "https://example.com/image2.png",
      "vector": [0.234, 0.567, 0.890, ...]
    }
  ]
}
```

### ModalitÃ  Testo (mode="text")

#### Richiesta

```json
{
  "input": {
    "mode": "text",
    "payload": {
      "text": "bikini top with floral pattern",
      "id": "query_1"
    }
  }
}
```

#### Risposta di Successo

```json
{
  "status": "OK",
  "embeddings": [
    {
      "text_id": "query_1",
      "vector": [0.123, 0.456, 0.789, ...]
    }
  ]
}
```

**Nota**: Gli embedding di testo e immagini sono nello stesso spazio vettoriale (stessa dimensione D) e possono essere usati per similarity search cross-modal. Puoi confrontare un embedding di testo con embedding di immagini usando cosine similarity.

### Risposta con Errori Parziali

```json
{
  "embeddings": [
    {
      "image_url": "https://example.com/image1.jpg",
      "vector": [0.123, 0.456, ...]
    }
  ],
  "warnings": {
    "failed_urls": [
      {
        "url": "https://example.com/bad.jpg",
        "error": "404 Not Found"
      }
    ],
    "message": "1 immagine/i non sono state elaborate"
  }
}
```

### Esempio con cURL - Immagini

```bash
curl -X POST https://api.runpod.ai/v2/{endpoint_id}/runsync \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "input": {
      "mode": "images",
      "payload": [
        "https://www.samelosangeles.com/cdn/shop/files/Same1260.jpg"
      ]
    }
  }'
```

### Esempio con cURL - Testo

```bash
curl -X POST https://api.runpod.ai/v2/{endpoint_id}/runsync \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "input": {
      "mode": "text",
      "payload": {
        "text": "bikini top with floral pattern",
        "id": "query_1"
      }
    }
  }'
```

### Esempio con Python - Immagini

```python
import requests

endpoint_id = "your-endpoint-id"
api_key = "your-api-key"

response = requests.post(
    f"https://api.runpod.ai/v2/{endpoint_id}/runsync",
    headers={
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    },
    json={
        "input": {
            "mode": "images",
            "payload": [
                "https://example.com/image1.jpg",
                "https://example.com/image2.jpg"
            ]
        }
    }
)

result = response.json()
for embedding in result["embeddings"]:
    print(f"URL: {embedding['image_url']}")
    print(f"Vector dimension: {len(embedding['vector'])}")
```

### Esempio con Python - Testo

```python
import requests

endpoint_id = "your-endpoint-id"
api_key = "your-api-key"

response = requests.post(
    f"https://api.runpod.ai/v2/{endpoint_id}/runsync",
    headers={
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    },
    json={
        "input": {
            "mode": "text",
            "payload": {
                "text": "bikini top with floral pattern",
                "id": "query_1"
            }
        }
    }
)

result = response.json()
for embedding in result["embeddings"]:
    print(f"Text ID: {embedding['text_id']}")
    print(f"Vector dimension: {len(embedding['vector'])}")
```

## ğŸ” Similarity Search

Gli embedding sono normalizzati L2, quindi puoi usare **dot product** per cosine similarity.

### Similarity Search Immagine-Immagine

```python
import numpy as np

# Embedding normalizzati L2
emb1 = np.array(result["embeddings"][0]["vector"])
emb2 = np.array(result["embeddings"][1]["vector"])

# Cosine similarity = dot product (perchÃ© normalizzati)
similarity = np.dot(emb1, emb2)

# Oppure usa cosine_similarity di sklearn
from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity([emb1], [emb2])[0][0]
```

### Similarity Search Cross-Modal (Testo-Immagine)

Gli embedding di testo e immagini sono nello stesso spazio vettoriale, quindi puoi confrontarli direttamente:

```python
import numpy as np
import requests

endpoint_id = "your-endpoint-id"
api_key = "your-api-key"

# 1. Genera embedding per una query testuale
text_response = requests.post(
    f"https://api.runpod.ai/v2/{endpoint_id}/runsync",
    headers={
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    },
    json={
        "input": {
            "mode": "text",
            "payload": {
                "text": "bikini top with floral pattern",
                "id": "query_1"
            }
        }
    }
)
text_embedding = np.array(text_response.json()["embeddings"][0]["vector"])

# 2. Genera embedding per immagini
images_response = requests.post(
    f"https://api.runpod.ai/v2/{endpoint_id}/runsync",
    headers={
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    },
    json={
        "input": {
            "mode": "images",
            "payload": [
                "https://example.com/image1.jpg",
                "https://example.com/image2.jpg"
            ]
        }
    }
)

# 3. Calcola similarity tra testo e ogni immagine
for img_embedding in images_response.json()["embeddings"]:
    img_vector = np.array(img_embedding["vector"])
    similarity = np.dot(text_embedding, img_vector)  # Cosine similarity
    print(f"Image: {img_embedding['image_url']}, Similarity: {similarity:.4f}")
```

**Nota**: La similarity va da -1 (completamente diversi) a 1 (identici). Valori piÃ¹ alti indicano maggiore similaritÃ  semantica tra il testo e l'immagine.

## ğŸ“Š Specifiche Tecniche

- **Dimensione embedding**: Dipende dal modello (tipicamente 768-1152 per SigLIP2 large)
- **Formato**: Array di float32 normalizzati L2
- **Batch size**: Supporta batch di qualsiasi dimensione (limitato solo da memoria GPU)
- **Timeout download**: 10 secondi per immagine
- **Risoluzione input**: 512x512 pixel (gestito automaticamente dal processor)

## ğŸ› Troubleshooting

### Errore: "CUDA out of memory"

**Causa**: Batch troppo grande o modello troppo pesante per la GPU

**Soluzione**:

- Riduci il numero di immagini per batch
- Usa GPU con piÃ¹ VRAM (24GB+ invece di 16GB)

### Errore: "Model not found"

**Causa**: Modello non scaricato da Hugging Face

**Soluzione**:

- Verifica connessione internet nel container
- Controlla che `MODEL_NAME` in `src/model.py` sia corretto
- Il modello viene scaricato automaticamente al primo utilizzo

### Embedding inconsistenti

**Causa**: Possibile problema con normalizzazione L2

**Soluzione**:

- Verifica che gli embedding abbiano norma ~1.0
- Controlla che non ci siano vettori nulli

## ğŸ“ Note

- Il modello viene scaricato automaticamente da Hugging Face al primo utilizzo
- La cache del modello viene salvata nel container (persiste tra richieste)
- Il primo utilizzo dopo deploy Ã¨ piÃ¹ lento (download modello)
- Le richieste successive sono molto veloci (modello giÃ  in memoria)

## ğŸ”— Riferimenti

- [RunPod Serverless Documentation](https://docs.runpod.io/serverless)
- [SigLIP2 Paper](https://arxiv.org/abs/2502.14786)
- [Hugging Face SigLIP2](https://huggingface.co/google/siglip2-large-patch16-512)
